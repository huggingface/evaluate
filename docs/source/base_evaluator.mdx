# Using the evaluator with transformers models

The `Evaluator` abstraction allows to evaluate models wrapped in a pipeline, responsible for handling all preprocessing and post-processing. Out-of-the-box, `Evaluator` supports transformers pipelines for the supported tasks, but custom pipelines can be passed, as showcased in [LINK].

Currently supported tasks are:
- `"image-classification"`: will use the [`ImageClassificationEvaluator`].
- `"question-answering"`: will use the [`QuestionAnsweringEvaluator`].
- `"text-classification"` (alias `"sentiment-analysis"` available): will use the [`TextClassificationEvaluator`].
- `"token-classification"`: will use the [`TokenClassificationEvaluator`].

Each task has its own set of requirements for the dataset format and pipeline output, make sure to check them out for your custom use case!

## Using a model from the Hub

Models from the Hugging Face Hub can be loaded out-of-the-box just given the namespace of the model on the Hub.

```python
from datasets import load_dataset
from evaluate import evaluator

task_evaluator = evaluator("question-answering")

data = load_dataset("squad", split="validation[:100]")
eval_results = task_evaluator.compute(
    model_or_pipeline="distilbert-base-uncased-distilled-squad",
    data=data,
    metric="squad"
)
```

Results are as follow:

```python
{
    'exact_match': 84.0,
    'f1': 88.53333333333335,
    'latency_in_seconds': 0.009725173320002795,
    'samples_per_second': 102.82593092127149,
    'total_time_in_seconds': 0.9725173320002796
}
```

Note that evaluation results include both the requested metric, and information about the time it took to obtain predictions through the pipeline. These additional time metrics give an useful first information on model speed as inference.

## Using a local model

Local models respecting the format from `save_pretrained()` (LINK) can be used as well! In this case, the model needs to be loaded into a pipeline first:

```python

from datasets import load_dataset
from evaluate import evaluator
from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained("/path/to/model/folder/")
tokenizer = AutoTokenizer.from_pretrained("/path/to/model/folder/")

pipe = pipeline(task="token-classification", model=model, tokenizer=tokenizer)

data = load_dataset("imdb", split="test").shuffle(seed=42).select(range(1000))

task_evaluator = evaluator("text-classification")
eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="seqeval",
    label_mapping=pipe.model.config.label2id
)
```

## Combining several metrics

The evaluator can perform evaluation on several metrics at once using `combine`.

TODO use e.g. text-classification here?

```python
# TODO EXTEND
from datasets import load_dataset
from evaluate import evaluator

task_evaluator = evaluator("text-classification")

data = load_dataset("imdb", split="test").shuffle(seed=42).select(range(1000))
eval_results = task_evaluator.compute(
    model_or_pipeline="lvwerra/distilbert-imdb",
    data=data,
    metric="accuracy",
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)
```

## Benchmarking several models

Here is an example where several models can be compared thanks to the `Evaluator` in only a few lines of code, abstracting away the preprocessing, inference, postprocessing, metric computation:

```python
import pandas as pd
import torch
import traceback
from tabulate import tabulate

from datasets import load_dataset
from evaluate import evaluator
from tqdm import tqdm
from transformers import pipeline

models = [
    "xlm-roberta-large-finetuned-conll03-english",
    "dbmdz/bert-large-cased-finetuned-conll03-english",
    "elastic/distilbert-base-uncased-finetuned-conll03-english",
    "dbmdz/electra-large-discriminator-finetuned-conll03-english",
    "gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner",
    "philschmid/distilroberta-base-ner-conll2003",
    "Jorgeutd/albert-base-v2-finetuned-ner",
]
models.sort()

device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

data = load_dataset("conll2003", split="validation")

num_samples = 1000
if num_samples != "all":
    data = data.shuffle(seed=42).select(range(num_samples))

task_evaluator = evaluator("token-classification")

full_results = []
evaluated_models = []
for model in tqdm(models):
    try:
        print(model)
        pipe = pipeline(task="token-classification", model=model, device=device)

        eval_results = task_evaluator.compute(
            model_or_pipeline=pipe, data=data, metric="seqeval", join_by=" "
        )

        full_results.append(eval_results)
        evaluated_models.append(model)
    except Exception as e:
        print(model, e)
        print(traceback.format_exc())

df = pd.DataFrame(full_results, index=evaluated_models)
df_filtered = df[["overall_f1", "overall_accuracy", "total_time_in_seconds", "samples_per_second", "latency_in_seconds"]]

print(df_filtered)
print("----")
print(tabulate(df_filtered, tablefmt="pipe", headers="keys", floatfmt=".3f"))
```

Results:

|   model                                                            |   overall_f1 |   overall_accuracy |   total_time_in_seconds |   samples_per_second |   latency_in_seconds |
|:-------------------------------------------------------------------|-------------:|-------------------:|------------------------:|---------------------:|---------------------:|
| Jorgeutd/albert-base-v2-finetuned-ner                              |        0.941 |              0.989 |                   4.515 |              221.468 |                0.005 |
| dbmdz/bert-large-cased-finetuned-conll03-english                   |        0.962 |              0.881 |                  11.648 |               85.850 |                0.012 |
| dbmdz/electra-large-discriminator-finetuned-conll03-english        |        0.965 |              0.881 |                  11.456 |               87.292 |                0.011 |
| elastic/distilbert-base-uncased-finetuned-conll03-english          |        0.940 |              0.989 |                   2.318 |              431.378 |                0.002 |
| gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner |        0.947 |              0.991 |                   2.376 |              420.873 |                0.002 |
| philschmid/distilroberta-base-ner-conll2003                        |        0.961 |              0.994 |                   2.436 |              410.579 |                0.002 |
| xlm-roberta-large-finetuned-conll03-english                        |        0.969 |              0.882 |                  11.996 |               83.359 |                0.012 |

<Tip warning={true}>

The sole results from the `Evaluator` should probably not be trusted to compare model performances. Outside of the architecture, many parameters may influence performances:

* Hyperparameters used during training
* Training data (some models may have been trained on more data than others)
* Model size
* Training time

Moreover, the time performances should only be considered as a first proxy, as they include all the processing that goes on in the pipeline. This may include tokenizing, post-processing, that may be different depending on the model, and may not reflect the exact model time performances. Especially, depending on the usage of slow or fast tokenizers, the results may be signifiicantly impacted.

</Tip>


## Evaluator task-specific arguments

Depending on the task, some task-specific arguments must be set. It is therefore best to check LINK for each task.

As an example, pipelines for classification tasks may ouput a label as a string, as in sentiment analysis between `negative`, `neutral` and `positive` sentences. To map these predictions to an id, we need an argument `label_mapping` mapping strings to numbers. In the case of transformers models, you will typically want to pass `pipe.model.config.label2id`.

However, beware that some models on the Hub have a label mapping that do not match with the dataset mapping! This may be that there is even no `label2id`, or that the labels are uninformative (`LABEL_0`, `LABEL_1`, etc.), as for [this model](https://huggingface.co/ArBert/albert-base-v2-finetuned-ner/blob/main/config.json).

In such case, it is best to inspect the dataset features in `data.features`, and, if possible, to use (Dataset.align_labels_with_mapping)[https://huggingface.co/docs/datasets/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping] to align the dataset labels with the model's outputs.

An other example is for `token-classification`, as in named entity recoginition (NER) or part of speech tagging (POS), where the common dataset structure assumes a list of words to be tagged as an input. Pipelines assume a single string as an input, and the list of word needs to be joined into a single string, using the `join_by` parameter, defaulting to `" "` for space-separated languages.

## Confidence intervals

TODO https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html.

## Handling large datasets

The evaluator can be used on large datasets! Below, an example shows how to use it on ImageNet-1k for image classification. Beware that this example will require to download ~150 GB.

Without specifying a device, the default will be the first GPU if available, else CPU.

```python
from datasets import load_dataset
from evaluate import evaluator
from transformers import pipeline

data = load_dataset("imagenet-1k", split="validation", use_auth_token=True)
data = data.shuffle(seed=42).select(range(500))

pipe = pipeline(
    task="image-classification",
    model="facebook/deit-small-distilled-patch16-224"
)

task_evaluator = evaluator("image-classification")
eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="accuracy",
    label_mapping=pipe.model.config.label2id
)
```
