{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9d201d-3b86-47d0-8b09-50e07f63bcac",
   "metadata": {},
   "source": [
    "## AdvGlue Evaluation Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae9f47-8c8c-4cd3-adba-65708af8221c",
   "metadata": {},
   "source": [
    "This HuggingFace `evaluate.EvaluationSuite` compares the GLUE results with Adversarial GLUE (AdvGLUE), a multi-task benchmark meauree the vulnerabilities of modern large-scal \n",
    "language models under various types of adversarial attacks.\n",
    "\n",
    "```\n",
    "@inproceedings{wang2021adversarial,\n",
    "title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models},\n",
    "author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},\n",
    "booktitle={Advances in Neural Information Processing Systems},\n",
    "year={2021}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cded5-c0fd-4b52-95f1-acc1af5fb8cd",
   "metadata": {},
   "source": [
    "Let us begin by writing our custom `EvaluateSuite` to evaluate hugging face's `glue` metric both against `glue` and `adv_glue` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734a81d-08f7-4707-a179-bcc005b2f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile adv_glue.py\n",
    "\n",
    "import evaluate\n",
    "from evaluate import EvaluationSuite, push_to_hub\n",
    "from evaluate.evaluation_suite import SubTask\n",
    "from evaluate.visualization import radar_plot\n",
    "from intel_evaluate_extension.model_card_suite import ModelCardSuiteResults\n",
    "\n",
    "_HEADER =  \"GLUE/AdvGlue Evaluation Results\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\n",
    "The suite compares the GLUE results with Adversarial GLUE (AdvGLUE), a multi-task benchmark \n",
    "meaure the vulnerabilities of modern large-scale language models under various types of adversarial attacks.\"\"\"\n",
    "        \n",
    "class Suite(ModelCardSuiteResults):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.preprocessor = lambda x: {\"text\": x[\"text\"].lower()}\n",
    "        self.result_keys = [\"accuracy\", \"f1\"]\n",
    "        self.summary = _DESCRIPTION\n",
    "        self.header = _HEADER\n",
    "        self.suite = [\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"glue\",\n",
    "                subset=\"sst2\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"sentence\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"sst2\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0.0,\n",
    "                        \"LABEL_1\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"adv_glue\",\n",
    "                subset=\"adv_sst2\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"sentence\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"sst2\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0.0,\n",
    "                        \"LABEL_1\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"glue\",\n",
    "                subset=\"qqp\",\n",
    "                split=\"validation[:5]\",\n",
    "\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"question1\",\n",
    "                    \"second_input_column\": \"question2\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"qqp\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"adv_glue\",\n",
    "                subset=\"adv_qqp\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"question1\",\n",
    "                    \"second_input_column\": \"question2\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"qqp\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"glue\",\n",
    "                subset=\"qnli\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"question\",\n",
    "                    \"second_input_column\": \"sentence\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"qnli\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"adv_glue\",\n",
    "                subset=\"adv_qnli\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"question\",\n",
    "                    \"second_input_column\": \"sentence\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"qnli\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"glue\",\n",
    "                subset=\"rte\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"sentence1\",\n",
    "                    \"second_input_column\": \"sentence2\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"rte\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"adv_glue\",\n",
    "                subset=\"adv_rte\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"sentence1\",\n",
    "                    \"second_input_column\": \"sentence2\",\n",
    "                    \"label_column\": \"label\",\n",
    "                    \"config_name\": \"rte\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"glue\",\n",
    "                subset=\"mnli\",\n",
    "                split=\"validation_mismatched[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"premise\",\n",
    "                    \"second_input_column\": \"hypothesis\",\n",
    "                    \"config_name\": \"mnli\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1,\n",
    "                        \"LABEL_2\": 2\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            SubTask(\n",
    "                task_type=\"text-classification\",\n",
    "                data=\"adv_glue\",\n",
    "                subset=\"adv_mnli\",\n",
    "                split=\"validation[:5]\",\n",
    "                args_for_task={\n",
    "                    \"metric\": \"glue\",\n",
    "                    \"input_column\": \"premise\",\n",
    "                    \"second_input_column\": \"hypothesis\",\n",
    "                    \"config_name\": \"mnli\",\n",
    "                    \"label_mapping\": {\n",
    "                        \"LABEL_0\": 0,\n",
    "                        \"LABEL_1\": 1,\n",
    "                        \"LABEL_2\": 2\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def process_results(self, results):\n",
    "        radar_data = [\n",
    "            {\"accuracy \" + result[\"task_name\"].split(\"/\")[-1]: \n",
    "             result[\"accuracy\"] for result in results[::2]},\n",
    "            {\"accuracy \" + result[\"task_name\"].replace(\"adv_\", \"\").split(\"/\")[-1]: \n",
    "             result[\"accuracy\"] for result in results[1::2]}]\n",
    "        return radar_plot(radar_data, ['GLUE', 'AdvGLUE'])\n",
    "\n",
    "    def plot_results(self, results, model_or_pipeline):\n",
    "        radar_data = self.process_results(results)\n",
    "        graphic = radar_plot(radar_data, ['GLUE ' + model_or_pipeline,  'AdvGLUE ' + model_or_pipeline])\n",
    "        return graphic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcc6d5-7381-498d-8fb7-b39998f1f335",
   "metadata": {},
   "source": [
    "## Run EvaluationSuite to Compare GLUE/AdvGlue Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8875f99-9623-4e84-b75c-d3f856e7f705",
   "metadata": {},
   "source": [
    "Now we run our `EvaluationSuite` on the `gpt2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84027b8-19b8-4256-9f2f-efcbe88906d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from evaluate import EvaluationSuite\n",
    "\n",
    "suite = EvaluationSuite.load('adv_glue.py') # This can also be found in '../intelai_hub/suites/adv_glu'\n",
    "mc_results = suite.run(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830307da-edae-4437-b764-02fb25a37a7f",
   "metadata": {},
   "source": [
    "## Create a Custom Model Card Markdown Template for Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99754cc-ecb4-41b1-809d-0be04deebe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile custom_template.md\n",
    "---\n",
    "{{ card_data }}\n",
    "---\n",
    "\n",
    "# Model Card for MyCoolModel\n",
    "\n",
    "This model does this and that.\n",
    "\n",
    "## Results\n",
    "\n",
    "{% for result in results.collection %}\n",
    "{{result.header}}\n",
    "\n",
    "{{result.table}}\n",
    "\n",
    "{{result.graphic}}\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## Model Card Contact\n",
    "\n",
    "This model was created by [@{{ author }}](https://hf.co/{{author}})."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2963c3-39f6-458b-ad13-eaf19f94feb7",
   "metadata": {},
   "source": [
    "## Push Evaluation Results to Model Card on Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfcc0f-06a3-424f-bc86-48da6a1bbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData, EvalResult\n",
    "\n",
    "hub_path = \"...\" # insert name on hugging face hub\n",
    "\n",
    "# Download Card\n",
    "card = ModelCard.load(hub_path)\n",
    "# Generate new model card\n",
    "card = ModelCard.from_template(card.data, 'custom_template.md', results=mc_results)\n",
    "# Push model to hub\n",
    "card.push_to_hub(hub_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
